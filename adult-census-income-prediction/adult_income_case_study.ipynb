{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151d5f54",
   "metadata": {},
   "source": [
    "\n",
    "# Adult Census Income Prediction — ML Case Study\n",
    "\n",
    "**Objective:** Predict whether an individual's income exceeds $50K/year (binary classification) using the UCI Adult Census dataset.\n",
    "\n",
    "**Deliverable:** A single, self-contained notebook with:\n",
    "- Clean, well-commented code\n",
    "- Clear rationale for each decision\n",
    "- Inline visualizations for EDA and model evaluation\n",
    "- Two supervised models: **Logistic Regression** and **Naive Bayes** (no XGBoost)\n",
    "- Overfitting controls and fair evaluation on a true holdout set\n",
    "\n",
    "**Author:** Tanmay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42bde53",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Business Problem (Plain English)\n",
    "\n",
    "We want to forecast whether a person is a **high earner** (>$50K) using demographic and work-related inputs (age, education, occupation, hours per week, etc.).\n",
    "\n",
    "**Why businesses care:**\n",
    "- **Recruiting:** prioritize candidates for senior roles.\n",
    "- **Marketing:** segment and target premium products/services.\n",
    "- **Policy/Analytics:** quantify income disparities across demographics.\n",
    "\n",
    "**Success criteria:** Build a model with strong **precision/recall** on the >$50K class, good **ROC-AUC**, and clear explainability so stakeholders understand the drivers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6372fd9",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Assumptions & Guardrails\n",
    "\n",
    "- We treat the UCI `adult.data` as **train** and `adult.test` as a **true external test** set (as published).\n",
    "- We'll avoid data leakage by fitting transformers **only on train**.\n",
    "- We'll use simple, robust preprocessing: missing value handling, one-hot encoding for categoricals, and appropriate scaling depending on model.\n",
    "- We will not use black-box gradient boosting here; requested focus is **Logistic Regression** and **Naive Bayes**.\n",
    "- We care about **calibrated probabilities** (for business thresholds) and **interpretability** (feature influence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d312f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Imports & Configuration\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling & Evaluation\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, accuracy_score)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# For feature names\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4eab7",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Access & Loading (UCI)\n",
    "\n",
    "We load the official Adult dataset directly from UCI:\n",
    "- Train: `adult.data`\n",
    "- Test: `adult.test`\n",
    "\n",
    "We also standardize column names and convert \"?\" to missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6afd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UCI URLs\n",
    "URL_TRAIN = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "URL_TEST  = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "\n",
    "# Schema from UCI (fixed order)\n",
    "COLS = [\n",
    "    \"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\"marital_status\",\n",
    "    \"occupation\",\"relationship\",\"race\",\"sex\",\"capital_gain\",\"capital_loss\",\n",
    "    \"hours_per_week\",\"native_country\",\"income\"\n",
    "]\n",
    "\n",
    "# Read with proper NA handling and whitespace trimming\n",
    "train_raw = pd.read_csv(URL_TRAIN, header=None, names=COLS, na_values=\" ?\", skipinitialspace=True)\n",
    "test_raw  = pd.read_csv(URL_TEST,  header=0,   names=COLS, na_values=\" ?\", skipinitialspace=True)\n",
    "\n",
    "print(train_raw.shape, test_raw.shape)\n",
    "train_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c82f4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Attribute Information (from UCI page)\n",
    "\n",
    "**Features** (abridged summary):\n",
    "- **age** (int)\n",
    "- **workclass** (Private, Self-emp, Gov, etc.)\n",
    "- **fnlwgt** (final sampling weight; often not predictive)\n",
    "- **education** (Bachelors, HS-grad, etc.)\n",
    "- **education_num** (years of education)\n",
    "- **marital_status** (Married, Never-married, etc.)\n",
    "- **occupation** (Tech-support, Craft-repair, etc.)\n",
    "- **relationship** (Wife, Husband, Not-in-family, etc.)\n",
    "- **race**, **sex**\n",
    "- **capital_gain**, **capital_loss**\n",
    "- **hours_per_week**\n",
    "- **native_country**\n",
    "- **income** (target: `<=50K` or `>50K`)\n",
    "\n",
    "We'll treat `income` as the binary target (1 for `>50K`, 0 for `<=50K`). We'll also consider whether **fnlwgt** helps or hurts; many practitioners drop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be144ec",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Data Cleaning\n",
    "\n",
    "Steps:\n",
    "1. Strip any trailing periods from test labels (they come as `<=50K.` / `>50K.`).\n",
    "2. Harmonize the target to 0/1.\n",
    "3. Handle missing values (`?` loaded as `NaN`). We'll **drop rows with missing critical categoricals** to keep things simple and reproducible.\n",
    "4. Optional: Drop `fnlwgt` (often noisy for prediction). We'll keep it first, then check importance and optionally drop it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ece01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_adult(df):\n",
    "    out = df.copy()\n",
    "    # normalize income strings (remove trailing periods, trim)\n",
    "    out[\"income\"] = out[\"income\"].astype(str).str.strip().str.replace(\".\", \"\", regex=False)\n",
    "    out[\"income\"] = out[\"income\"].map({\">50K\": 1, \"<=50K\": 0})\n",
    "    # Drop rows with any NA in common categoricals (simple and robust)\n",
    "    # We'll keep numeric NAs (there shouldn't be any in this dataset)\n",
    "    before = out.shape[0]\n",
    "    out = out.dropna()\n",
    "    after = out.shape[0]\n",
    "    print(f\"Dropped {before-after} rows with missing values.\")\n",
    "    return out\n",
    "\n",
    "train = clean_adult(train_raw)\n",
    "test  = clean_adult(test_raw)\n",
    "\n",
    "print(train[\"income\"].value_counts(normalize=True))\n",
    "print(test[\"income\"].value_counts(normalize=True))\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adc3e7",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We examine class balance and key numeric distributions.  \n",
    "*All charts use Matplotlib; one chart per figure, with default styles (no custom colors).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa13911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper to plot a simple bar chart for class balance\n",
    "def plot_class_balance(series, title):\n",
    "    counts = series.value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Income (0=<=50K, 1=>50K)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "plot_class_balance(train[\"income\"], \"Training Set: Class Balance\")\n",
    "plot_class_balance(test[\"income\"], \"Test Set: Class Balance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29035db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric distributions\n",
    "numeric_cols = [\"age\", \"education_num\", \"hours_per_week\", \"capital_gain\", \"capital_loss\"]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    train[col].hist(bins=40)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Relationship with target (boxplots)\n",
    "for col in [\"age\", \"hours_per_week\"]:\n",
    "    plt.figure()\n",
    "    train.boxplot(column=col, by=\"income\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.title(f\"{col} vs Income\")\n",
    "    plt.xlabel(\"Income (0=<=50K, 1=>50K)\")\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55d5c1",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Feature Engineering & Preprocessing\n",
    "\n",
    "We split features into **numeric** and **categorical** columns and build two pipelines:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  - Numeric: `StandardScaler`\n",
    "  - Categorical: `OneHotEncoder(handle_unknown=\"ignore\")`  \n",
    "  - Class weight balanced to mitigate skew.\n",
    "\n",
    "- **Naive Bayes (MultinomialNB)**  \n",
    "  - Naive Bayes expects **non-negative** features.  \n",
    "  - Numeric: `MinMaxScaler` to [0, 1]  \n",
    "  - Categorical: `OneHotEncoder`  \n",
    "  - We'll tune `alpha` via grid search.\n",
    "\n",
    "We will fit on **train** and evaluate on the **published test** set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify columns\n",
    "target_col = \"income\"\n",
    "feature_cols = [c for c in train.columns if c != target_col]\n",
    "\n",
    "numeric_features = train[feature_cols].select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = [c for c in feature_cols if c not in numeric_features]\n",
    "\n",
    "numeric_features, categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4724bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Column transformers\n",
    "ct_logit = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numeric_features),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_features)\n",
    "])\n",
    "\n",
    "ct_nb = ColumnTransformer([\n",
    "    (\"num\", MinMaxScaler(), numeric_features),  # non-negative\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_features)\n",
    "])\n",
    "\n",
    "# Pipelines\n",
    "pipe_logit = Pipeline([\n",
    "    (\"prep\", ct_logit),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, class_weight=\"balanced\", n_jobs=None))\n",
    "])\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    (\"prep\", ct_nb),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b1d20",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Model Training (with Cross-Validation)\n",
    "\n",
    "We use **GridSearchCV** with stratified 5-fold CV.  \n",
    "- **Metric**: ROC-AUC (robust for imbalanced classes)\n",
    "- We keep grids tight to avoid overfitting with excessive search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ad969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter grids\n",
    "param_grid_logit = {\n",
    "    \"clf__C\": [0.25, 1.0, 4.0],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__solver\": [\"lbfgs\"]  # stable & supports class_weight\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "gs_logit = GridSearchCV(pipe_logit, param_grid_logit, cv=cv, scoring=\"roc_auc\", n_jobs=-1, refit=True)\n",
    "gs_nb    = GridSearchCV(pipe_nb,    param_grid_nb,   cv=cv, scoring=\"roc_auc\", n_jobs=-1, refit=True)\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[target_col]\n",
    "\n",
    "gs_logit.fit(X_train, y_train)\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Logistic Regression params:\", gs_logit.best_params_, \"CV AUC:\", round(gs_logit.best_score_, 4))\n",
    "print(\"Best Naive Bayes params:\", gs_nb.best_params_, \"CV AUC:\", round(gs_nb.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bea842",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Evaluation on the External Test Set\n",
    "\n",
    "We report:\n",
    "- Accuracy, Precision, Recall, F1 (on >50K class)\n",
    "- ROC-AUC\n",
    "- Confusion Matrix\n",
    "- ROC and Precision-Recall curves (inline plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae16717",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Use predict_proba if available; else decision_function; else fallback to predicted labels for AUC (less ideal)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_proba = model.decision_function(X_test)\n",
    "        # Scale to [0,1] if needed\n",
    "        y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min() + 1e-12)\n",
    "    else:\n",
    "        y_proba = y_pred.astype(float)\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "    print(\"ROC-AUC:\", round(roc_auc_score(y_test, y_proba), 4))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.title(f\"ROC Curve — {name}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision)\n",
    "    plt.title(f\"Precision-Recall Curve — {name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test[target_col]\n",
    "\n",
    "best_logit = gs_logit.best_estimator_\n",
    "best_nb    = gs_nb.best_estimator_\n",
    "\n",
    "evaluate_model(\"Logistic Regression\", best_logit, X_test, y_test)\n",
    "evaluate_model(\"Naive Bayes (Multinomial)\", best_nb, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40610561",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Model Interpretability (Logistic Regression)\n",
    "\n",
    "We extract the top positive/negative coefficients to understand which features push predictions toward `>50K` or `<=50K`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7061799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_names_from_ct(ct, input_cols):\n",
    "    # Works for our specific ColumnTransformer definition\n",
    "    output_names = []\n",
    "    for name, transformer, cols in ct.transformers_:\n",
    "        if transformer == \"drop\":\n",
    "            continue\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            try:\n",
    "                names = transformer.get_feature_names_out(cols)\n",
    "            except TypeError:\n",
    "                names = transformer.get_feature_names_out()\n",
    "            output_names.extend(names)\n",
    "        else:\n",
    "            # passthrough or scaler without names\n",
    "            if isinstance(cols, list):\n",
    "                output_names.extend(cols)\n",
    "            else:\n",
    "                output_names.extend(input_cols if cols == \"remainder\" else [cols])\n",
    "    return output_names\n",
    "\n",
    "# Extract coefficients from the best logistic model\n",
    "logit_clf = best_logit.named_steps[\"clf\"]\n",
    "ct = best_logit.named_steps[\"prep\"]\n",
    "\n",
    "feature_names = get_feature_names_from_ct(ct, feature_cols)\n",
    "\n",
    "coef = logit_clf.coef_.ravel()\n",
    "coef_df = pd.DataFrame({\"feature\": feature_names, \"coef\": coef}).sort_values(\"coef\", ascending=False)\n",
    "\n",
    "top_k = 15\n",
    "top_pos = coef_df.head(top_k)\n",
    "top_neg = coef_df.tail(top_k).iloc[::-1]\n",
    "\n",
    "display(top_pos)\n",
    "display(top_neg)\n",
    "\n",
    "# Plot top positives\n",
    "plt.figure()\n",
    "plt.barh(top_pos[\"feature\"][::-1], top_pos[\"coef\"][::-1])\n",
    "plt.title(\"Top Positive Coefficients (Logistic Regression)\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# Plot top negatives\n",
    "plt.figure()\n",
    "plt.barh(top_neg[\"feature\"][::-1], top_neg[\"coef\"][::-1])\n",
    "plt.title(\"Top Negative Coefficients (Logistic Regression)\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a80af",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Discussion: Which Model Is Best and Why?\n",
    "\n",
    "We compare both models on **external test**:\n",
    "- If **Logistic Regression** shows higher ROC-AUC and balanced precision/recall, it wins for **interpretability** and **calibration**.\n",
    "- If **Multinomial Naive Bayes** is close or better, it's attractive for **speed** and **simplicity**, but may be less calibrated on numeric-heavy features.\n",
    "\n",
    "**Bias/Variance & Overfitting Controls**\n",
    "- Used a true holdout (published test file).\n",
    "- Simple preprocessing (no leakage). \n",
    "- Modest hyperparameter search with cross-validation (5-fold).\n",
    "- Regularized Logistic Regression.\n",
    "\n",
    "**Practical Recommendation**\n",
    "- Choose the model with **higher ROC-AUC** and operationally desirable **precision/recall** on the positive class.\n",
    "- For deployment, set a business threshold (probability cutoff) aligned with costs (false positives vs false negatives). Use the PR curve to pick a point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b9bfa",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Next Steps\n",
    "\n",
    "- Try **calibration** (Platt/Isotonic) and a **cost-sensitive threshold** tuned to business goals.\n",
    "- Explore **feature interactions** and drop **fnlwgt** if it hurts generalization.\n",
    "- Investigate **fairness metrics** (e.g., across `sex` or `race`) before real-world use.\n",
    "- Add **learning curve** diagnostics and **error analysis** on top contributing segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef8a82",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Reproducibility\n",
    "\n",
    "- Random states fixed where relevant.\n",
    "- All transformations encapsulated in scikit-learn **Pipelines**.\n",
    "- Clear separation of **train** (adult.data) vs **test** (adult.test).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
